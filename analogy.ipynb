{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anaogies with Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "For this exercise we will be asking the model the **Analogy** questions:\n",
    "\n",
    "### If Italy is to Italian then Spain is to? \t\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n",
    "\n",
    "### If India is to delhi then Japan is to? \t\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_ \n",
    "\n",
    "### If man is to  woman then boy is to? \t\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_  \n",
    "\n",
    "### If small is to  smaller then large is to? \t\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_   \n",
    "\n",
    "Using a pre-trained GloVe word vector I will make a model that will accurately answer the questions above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## GloVe: Global Vectors for Word Representation\n",
    "https://nlp.stanford.edu/pubs/glove.pdf\n",
    "\n",
    "GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the glove model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r',encoding='utf8') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        \n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "            \n",
    "    return words, word_to_vec_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words, word_to_vec_map = read_glove_vecs('glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set of words available in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_words = list(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 400000 words available\n"
     ]
    }
   ],
   "source": [
    "print('There are '+str(len(sorted_words))+' words available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted_words = np.array(sorted_words).reshape([1000,400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>61508</td>\n",
       "      <td>girouard</td>\n",
       "      <td>hamoked</td>\n",
       "      <td>capitals</td>\n",
       "      <td>seder</td>\n",
       "      <td>2328</td>\n",
       "      <td>hopsin</td>\n",
       "      <td>cherkessia</td>\n",
       "      <td>antigone</td>\n",
       "      <td>tenser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>wagp</td>\n",
       "      <td>bitterlich</td>\n",
       "      <td>thot</td>\n",
       "      <td>3,969</td>\n",
       "      <td>werke</td>\n",
       "      <td>3,7</td>\n",
       "      <td>thonet</td>\n",
       "      <td>caymmi</td>\n",
       "      <td>41.83</td>\n",
       "      <td>halflife</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>scheving</td>\n",
       "      <td>powderly</td>\n",
       "      <td>toribio</td>\n",
       "      <td>sisman</td>\n",
       "      <td>admissions</td>\n",
       "      <td>someplace</td>\n",
       "      <td>mcmeans</td>\n",
       "      <td>bundesstra√üen</td>\n",
       "      <td>ypsilon</td>\n",
       "      <td>spiciness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>euro595</td>\n",
       "      <td>memorably</td>\n",
       "      <td>non-essential</td>\n",
       "      <td>maculata</td>\n",
       "      <td>batalov</td>\n",
       "      <td>lms</td>\n",
       "      <td>deyong</td>\n",
       "      <td>swpa</td>\n",
       "      <td>4-cd</td>\n",
       "      <td>hendrickx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>reincorporation</td>\n",
       "      <td>balkline</td>\n",
       "      <td>cooler</td>\n",
       "      <td>subsides</td>\n",
       "      <td>grandkids</td>\n",
       "      <td>inside-forward</td>\n",
       "      <td>mingus</td>\n",
       "      <td>byplay</td>\n",
       "      <td>hemudu</td>\n",
       "      <td>stocco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>3,229</td>\n",
       "      <td>mangano</td>\n",
       "      <td>tdcs</td>\n",
       "      <td>ppf</td>\n",
       "      <td>cullen@globe.com</td>\n",
       "      <td>dispositions</td>\n",
       "      <td>kitto</td>\n",
       "      <td>re-surfaced</td>\n",
       "      <td>renin-angiotensin</td>\n",
       "      <td>defrantz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>sandretti</td>\n",
       "      <td>tuckwell</td>\n",
       "      <td>ckgm</td>\n",
       "      <td>alodia</td>\n",
       "      <td>fioravanti</td>\n",
       "      <td>shenay</td>\n",
       "      <td>wanchai</td>\n",
       "      <td>siewierz</td>\n",
       "      <td>filer</td>\n",
       "      <td>satpayev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>hirschsprung</td>\n",
       "      <td>municipales</td>\n",
       "      <td>july/august</td>\n",
       "      <td>krupskaya</td>\n",
       "      <td>pittermann</td>\n",
       "      <td>yearslong</td>\n",
       "      <td>mapudungun</td>\n",
       "      <td>wollman</td>\n",
       "      <td>tubau</td>\n",
       "      <td>clayderman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>cctlds</td>\n",
       "      <td>mithradates</td>\n",
       "      <td>80.33</td>\n",
       "      <td>thompson</td>\n",
       "      <td>86.95</td>\n",
       "      <td>bernie</td>\n",
       "      <td>esoterica</td>\n",
       "      <td>itaim</td>\n",
       "      <td>1500cc</td>\n",
       "      <td>straps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>charismatic</td>\n",
       "      <td>dairy</td>\n",
       "      <td>hoopoes</td>\n",
       "      <td>abida</td>\n",
       "      <td>frankton</td>\n",
       "      <td>1269</td>\n",
       "      <td>fotis</td>\n",
       "      <td>mcconaughy</td>\n",
       "      <td>-9:00</td>\n",
       "      <td>alltime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>interoceanic</td>\n",
       "      <td>lhota</td>\n",
       "      <td>0.7-percent</td>\n",
       "      <td>3-and-0</td>\n",
       "      <td>infoworld</td>\n",
       "      <td>ferncliff</td>\n",
       "      <td>misspell</td>\n",
       "      <td>neema</td>\n",
       "      <td>kinderen</td>\n",
       "      <td>red-flagged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>tood</td>\n",
       "      <td>104</td>\n",
       "      <td>orasje</td>\n",
       "      <td>4,6</td>\n",
       "      <td>378</td>\n",
       "      <td>econometrics</td>\n",
       "      <td>lavric</td>\n",
       "      <td>greenwald</td>\n",
       "      <td>divisibility</td>\n",
       "      <td>kshsaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>gaboriau</td>\n",
       "      <td>montecasino</td>\n",
       "      <td>herrs</td>\n",
       "      <td>goldblatt</td>\n",
       "      <td>32.34</td>\n",
       "      <td>zizzo</td>\n",
       "      <td>trengove</td>\n",
       "      <td>92.93</td>\n",
       "      <td>ransford</td>\n",
       "      <td>noz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>16:46</td>\n",
       "      <td>philippine</td>\n",
       "      <td>3,448</td>\n",
       "      <td>marshland</td>\n",
       "      <td>29,800</td>\n",
       "      <td>shoushan</td>\n",
       "      <td>taisen</td>\n",
       "      <td>.923</td>\n",
       "      <td>matviyenko</td>\n",
       "      <td>1.174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>pinhey</td>\n",
       "      <td>checkpoints</td>\n",
       "      <td>trinitarian</td>\n",
       "      <td>repurposed</td>\n",
       "      <td>lozier</td>\n",
       "      <td>kyi</td>\n",
       "      <td>godfred</td>\n",
       "      <td>1,670</td>\n",
       "      <td>true-crime</td>\n",
       "      <td>jessicah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>michalis</td>\n",
       "      <td>vegalta</td>\n",
       "      <td>v≈©</td>\n",
       "      <td>zouerat</td>\n",
       "      <td>√∂lvir</td>\n",
       "      <td>124.06</td>\n",
       "      <td>zhixiang</td>\n",
       "      <td>brancacci</td>\n",
       "      <td>impounding</td>\n",
       "      <td>redenbaugh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>piscopo</td>\n",
       "      <td>flutist</td>\n",
       "      <td>destabilising</td>\n",
       "      <td>civically</td>\n",
       "      <td>tuymans</td>\n",
       "      <td>cementitious</td>\n",
       "      <td>sfgate.com</td>\n",
       "      <td>travertine</td>\n",
       "      <td>skycable</td>\n",
       "      <td>stipe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>wolfhound</td>\n",
       "      <td>queenie</td>\n",
       "      <td>omct</td>\n",
       "      <td>ivillage</td>\n",
       "      <td>neurontin</td>\n",
       "      <td>skeletons</td>\n",
       "      <td>rattles</td>\n",
       "      <td>beguelin</td>\n",
       "      <td>mobilio</td>\n",
       "      <td>alfr√©d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>computerize</td>\n",
       "      <td>progressed</td>\n",
       "      <td>hallett</td>\n",
       "      <td>moonlighting</td>\n",
       "      <td>fauvel</td>\n",
       "      <td>begemann</td>\n",
       "      <td>rugian</td>\n",
       "      <td>musoke</td>\n",
       "      <td>wagoneer</td>\n",
       "      <td>fmct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>library/media</td>\n",
       "      <td>sitruk</td>\n",
       "      <td>allos</td>\n",
       "      <td>verticillata</td>\n",
       "      <td>fereydoun</td>\n",
       "      <td>picolinate</td>\n",
       "      <td>nepeta</td>\n",
       "      <td>gaylard</td>\n",
       "      <td>1,269</td>\n",
       "      <td>wolds</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 100          101            102           103  \\\n",
       "400            61508     girouard        hamoked      capitals   \n",
       "401             wagp   bitterlich           thot         3,969   \n",
       "402         scheving     powderly        toribio        sisman   \n",
       "403          euro595    memorably  non-essential      maculata   \n",
       "404  reincorporation     balkline         cooler      subsides   \n",
       "405            3,229      mangano           tdcs           ppf   \n",
       "406        sandretti     tuckwell           ckgm        alodia   \n",
       "407     hirschsprung  municipales    july/august     krupskaya   \n",
       "408           cctlds  mithradates          80.33      thompson   \n",
       "409      charismatic        dairy        hoopoes         abida   \n",
       "410     interoceanic        lhota    0.7-percent       3-and-0   \n",
       "411             tood          104         orasje           4,6   \n",
       "412         gaboriau  montecasino          herrs     goldblatt   \n",
       "413            16:46   philippine          3,448     marshland   \n",
       "414           pinhey  checkpoints    trinitarian    repurposed   \n",
       "415         michalis      vegalta             v≈©       zouerat   \n",
       "416          piscopo      flutist  destabilising     civically   \n",
       "417        wolfhound      queenie           omct      ivillage   \n",
       "418      computerize   progressed        hallett  moonlighting   \n",
       "419    library/media       sitruk          allos  verticillata   \n",
       "\n",
       "                  104             105         106            107  \\\n",
       "400             seder            2328      hopsin     cherkessia   \n",
       "401             werke             3,7      thonet         caymmi   \n",
       "402        admissions       someplace     mcmeans  bundesstra√üen   \n",
       "403           batalov             lms      deyong           swpa   \n",
       "404         grandkids  inside-forward      mingus         byplay   \n",
       "405  cullen@globe.com    dispositions       kitto    re-surfaced   \n",
       "406        fioravanti          shenay     wanchai       siewierz   \n",
       "407        pittermann       yearslong  mapudungun        wollman   \n",
       "408             86.95          bernie   esoterica          itaim   \n",
       "409          frankton            1269       fotis     mcconaughy   \n",
       "410         infoworld       ferncliff    misspell          neema   \n",
       "411               378    econometrics      lavric      greenwald   \n",
       "412             32.34           zizzo    trengove          92.93   \n",
       "413            29,800        shoushan      taisen           .923   \n",
       "414            lozier             kyi     godfred          1,670   \n",
       "415             √∂lvir          124.06    zhixiang      brancacci   \n",
       "416           tuymans    cementitious  sfgate.com     travertine   \n",
       "417         neurontin       skeletons     rattles       beguelin   \n",
       "418            fauvel        begemann      rugian         musoke   \n",
       "419         fereydoun      picolinate      nepeta        gaylard   \n",
       "\n",
       "                   108          109  \n",
       "400           antigone       tenser  \n",
       "401              41.83     halflife  \n",
       "402            ypsilon    spiciness  \n",
       "403               4-cd    hendrickx  \n",
       "404             hemudu       stocco  \n",
       "405  renin-angiotensin     defrantz  \n",
       "406              filer     satpayev  \n",
       "407              tubau   clayderman  \n",
       "408             1500cc       straps  \n",
       "409              -9:00      alltime  \n",
       "410           kinderen  red-flagged  \n",
       "411       divisibility       kshsaa  \n",
       "412           ransford          noz  \n",
       "413         matviyenko        1.174  \n",
       "414         true-crime     jessicah  \n",
       "415         impounding   redenbaugh  \n",
       "416           skycable        stipe  \n",
       "417            mobilio       alfr√©d  \n",
       "418           wagoneer         fmct  \n",
       "419              1,269        wolds  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(sorted_words).iloc[400:420,100:110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word to Vec Map (Embedding Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our embedding has a dimension of 50\n"
     ]
    }
   ],
   "source": [
    "print('Our embedding has a dimension of '+str(pd.DataFrame(word_to_vec_map).shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief Explanation of Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a simple example of Emedding. Basically, each row is an attribute which can be anything from gender, color, smell etc. If the word is said to satisfy this attribute, it will be labeled a number between zero and one. The more it satisfies the attribute the closer it is to one. If it is the opposite of the attribute, it will be labeled a number from zero to negative one. The more opposite it is, the closer it is to negative one. In addition, a row can be a combination of a number of attributes and an embedding that has more dimension(rows) can store more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|        | husband|wife  | \n",
    "| :----| :----: |:----:| \n",
    "|Attribute 1: Male  |    0.98   | -.99   | \n",
    "|Attribute 2|  0.01  | 0.06 |  \n",
    "|Attribute 3| -0.03  | 0.02 |   \n",
    "| ...|  ...  |  ... |   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see below, the real data wont make much sense because it is not labeled and each row can be a combination of different attributes that we dont know. A lot of insights can be gotten if each row is labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>husband</th>\n",
       "      <th>wife</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.195400</td>\n",
       "      <td>0.64556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.607380</td>\n",
       "      <td>-0.46543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.008143</td>\n",
       "      <td>-0.51727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.079340</td>\n",
       "      <td>-0.15117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.372040</td>\n",
       "      <td>0.42836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.254510</td>\n",
       "      <td>-0.16713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.455280</td>\n",
       "      <td>-0.69901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     husband     wife\n",
       "7   0.195400  0.64556\n",
       "8  -0.607380 -0.46543\n",
       "9   0.008143 -0.51727\n",
       "10 -0.079340 -0.15117\n",
       "11  0.372040  0.42836\n",
       "12 -0.254510 -0.16713\n",
       "13 -0.455280 -0.69901"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(word_to_vec_map).loc[7:13,['husband','wife']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the distance between two words: Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of distance metrics avaialable (euclidean distance, manhattan distance etc.), but the most appropriate for this situation is the use of Cosine Similarity. The formula is as follows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](cossimilarity.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    distance = 0.0\n",
    "    \n",
    "    #dot product (numerator)\n",
    "    numerator = np.dot(a,b)\n",
    "    \n",
    "    #L2 norm:\n",
    "    l2_a = np.sqrt(np.sum(a**2,axis=0))\n",
    "    l2_b = np.sqrt(np.sum(b**2,axis=0))\n",
    "    \n",
    "    #denominator:\n",
    "    denominator = l2_a*l2_b\n",
    "\n",
    "    return numerator/denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity of \"husband\" and \"wife\" is 0.950674320925\n"
     ]
    }
   ],
   "source": [
    "husband = word_to_vec_map[\"husband\"]\n",
    "wife = word_to_vec_map[\"wife\"]\n",
    "print('Cosine Similarity of \"husband\" and \"wife\" is '+str(cosine_similarity(husband, wife)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words \"husband\" and \"wife\" are usually used in the same context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity of \"begin\" and \"start\" is 0.8819238975\n"
     ]
    }
   ],
   "source": [
    "husband = word_to_vec_map[\"begin\"]\n",
    "wife = word_to_vec_map[\"start\"]\n",
    "print('Cosine Similarity of \"begin\" and \"start\" is '+str(cosine_similarity(husband, wife)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words that are synonymous also have high Cosine Similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity of \"hot\" and \"cold\" is 0.801052788872\n"
     ]
    }
   ],
   "source": [
    "husband = word_to_vec_map[\"hot\"]\n",
    "wife = word_to_vec_map[\"cold\"]\n",
    "print('Cosine Similarity of \"hot\" and \"cold\" is '+str(cosine_similarity(husband, wife)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even words that are opposite also have high Cosine Similarities because they are used in the same context. For the example above, they are used when talking about the temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity of \"sink\" and \"wisdom\" is 0.285277721749\n"
     ]
    }
   ],
   "source": [
    "husband = word_to_vec_map[\"sink\"]\n",
    "wife = word_to_vec_map[\"wisdom\"]\n",
    "print('Cosine Similarity of \"sink\" and \"wisdom\" is '+str(cosine_similarity(husband, wife)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words that are completely irrelevant will have low Cosine Similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analogy(word1, word2, word3, word_to_vec_map):\n",
    "\n",
    "    #Convert the words into lowercase\n",
    "    word1, word2, word3 = word1.lower(), word2.lower(), word3.lower()\n",
    "    \n",
    "    #Get the subset of the embedding for the word \n",
    "    embed_w1, embed_w2, embed_w3 = word_to_vec_map[word1],word_to_vec_map[word2],word_to_vec_map[word3]\n",
    "\n",
    "    #Get the words\n",
    "    words = word_to_vec_map.keys()\n",
    "    \n",
    "    # Initialize max_cosine_sim to a large negative number\n",
    "    max_cosine_sim = -100              \n",
    "    \n",
    "    # Initialize best_word with None\n",
    "    best_word = None                   \n",
    "\n",
    "    #Loop though the words\n",
    "    for word in words:        \n",
    "        # to avoid best_word being one of the input words, pass on them.\n",
    "        if word in [word1, word2, word3] :\n",
    "            continue\n",
    "        \n",
    "        # Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c)  \n",
    "        cosine_sim = cosine_similarity(np.subtract(embed_w2,embed_w1),np.subtract(word_to_vec_map[word],embed_w3) )\n",
    "        \n",
    "        # If the cosine_sim is more than the max_cosine_sim seen so far,\n",
    "            # then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word \n",
    "        if cosine_sim > max_cosine_sim:\n",
    "            max_cosine_sim = cosine_sim\n",
    "            best_word = word\n",
    "        \n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "italy -> italian :: spain -> spanish\n",
      "india -> delhi :: japan -> tokyo\n",
      "man -> woman :: boy -> girl\n",
      "small -> smaller :: large -> larger\n"
     ]
    }
   ],
   "source": [
    "triads_to_try = [('italy', 'italian', 'spain'), ('india', 'delhi', 'japan'), ('man', 'woman', 'boy'), ('small', 'smaller', 'large')]\n",
    "for triad in triads_to_try:\n",
    "    print ('{} -> {} :: {} -> {}'.format( *triad, analogy(*triad,word_to_vec_map)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
